{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbhY1aHbGaGH",
        "outputId": "993df283-db6d-41a3-f661-edde93ae6dc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello\n"
          ]
        }
      ],
      "source": [
        "print(\"hello\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"proc_ds.csv\", encoding=\"ISO-8859-1\")\n",
        "print(\"Dataset Loaded Successfully!\")\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "MXkmZ9t4HFeb",
        "outputId": "80464d6c-25d9-45c7-e3dc-6a0aea8850dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Loaded Successfully!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Grievance ID            Category  \\\n",
              "0        G0001  medical and health   \n",
              "1        G0002         maintenance   \n",
              "2        G0003         maintenance   \n",
              "3        G0004    train operations   \n",
              "4        G0005         maintenance   \n",
              "\n",
              "                               Grievance Description  \\\n",
              "0  My son is vomiting again and again. Age ï¿½ï¿½...   \n",
              "1  Train no .22987...Ajmer to Agra...windows not ...   \n",
              "2  Dear Team, Charging point of coach no. B6/42/M...   \n",
              "3  Train no 12360, hundreds of without ticket pas...   \n",
              "4  I'm in SS class My PNR is 2326704127 and this ...   \n",
              "\n",
              "                                   Customer Feedback Urgency Level  \\\n",
              "0  Thank you @RailwaySeva @RailMinIndia Due to yo...          high   \n",
              "1  Thanks for the prompt service..Problem has bee...        medium   \n",
              "2  Complaint has been Solved... Thanks for Quick ...        medium   \n",
              "3  Thanks a lot. 2 officers came and finally I go...          high   \n",
              "4  Thanks @RailMadad @RailwaySeva @DrmAjmer compl...        medium   \n",
              "\n",
              "                               Complaint Keywords  \n",
              "0                     son, vomiting, age 13 years  \n",
              "1                      windows, not having mirror  \n",
              "2                     charging point, not working  \n",
              "3  without ticket, passengers flooding everywhere  \n",
              "4                        windows, can't be locked  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-329d3773-4265-4aa6-b395-2cda858ff3dd\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Grievance ID</th>\n",
              "      <th>Category</th>\n",
              "      <th>Grievance Description</th>\n",
              "      <th>Customer Feedback</th>\n",
              "      <th>Urgency Level</th>\n",
              "      <th>Complaint Keywords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>G0001</td>\n",
              "      <td>medical and health</td>\n",
              "      <td>My son is vomiting again and again. Age ï¿½ï¿½...</td>\n",
              "      <td>Thank you @RailwaySeva @RailMinIndia Due to yo...</td>\n",
              "      <td>high</td>\n",
              "      <td>son, vomiting, age 13 years</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>G0002</td>\n",
              "      <td>maintenance</td>\n",
              "      <td>Train no .22987...Ajmer to Agra...windows not ...</td>\n",
              "      <td>Thanks for the prompt service..Problem has bee...</td>\n",
              "      <td>medium</td>\n",
              "      <td>windows, not having mirror</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>G0003</td>\n",
              "      <td>maintenance</td>\n",
              "      <td>Dear Team, Charging point of coach no. B6/42/M...</td>\n",
              "      <td>Complaint has been Solved... Thanks for Quick ...</td>\n",
              "      <td>medium</td>\n",
              "      <td>charging point, not working</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>G0004</td>\n",
              "      <td>train operations</td>\n",
              "      <td>Train no 12360, hundreds of without ticket pas...</td>\n",
              "      <td>Thanks a lot. 2 officers came and finally I go...</td>\n",
              "      <td>high</td>\n",
              "      <td>without ticket, passengers flooding everywhere</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>G0005</td>\n",
              "      <td>maintenance</td>\n",
              "      <td>I'm in SS class My PNR is 2326704127 and this ...</td>\n",
              "      <td>Thanks @RailMadad @RailwaySeva @DrmAjmer compl...</td>\n",
              "      <td>medium</td>\n",
              "      <td>windows, can't be locked</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-329d3773-4265-4aa6-b395-2cda858ff3dd')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-329d3773-4265-4aa6-b395-2cda858ff3dd button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-329d3773-4265-4aa6-b395-2cda858ff3dd');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e828b26d-39b6-4503-ac67-8ae1e3fc0931\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e828b26d-39b6-4503-ac67-8ae1e3fc0931')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e828b26d-39b6-4503-ac67-8ae1e3fc0931 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 703,\n  \"fields\": [\n    {\n      \"column\": \"Grievance ID\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 703,\n        \"samples\": [\n          \"G0967\",\n          \"G0195\",\n          \"G0055\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"maintenance\",\n          \"security\",\n          \"medical and health\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Grievance Description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 701,\n        \"samples\": [\n          \"@RailMinIndia confusion or misprint #whatever# but Chd to allp was worst noAC,ntClean,leaking in rain,noWater https://t.co/MLRJygEHmt\",\n          \"@RailMinIndia @sureshpprabhu PNR - 6756068777, AC not working, dirty towel and pillow cover.\",\n          \"@IRCTCofficial @WesternRly @Central_Railway  need colicaid medicine for my 6 year children. Please Help. PNR : 2535816255. Train number 11072. Train leave from manikpur few minutes back.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Customer Feedback\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 53,\n        \"samples\": [\n          \"Medical will be attend at TDL.\",\n          \"Thank you for your prompt action. Cleaning staff started attending toilets. I really appreciate your response. Best wishes to you and your team.\",\n          \"Thank you So much @IRCTCofficial for instant cleaning service in coach & Thank you also Dilipbhai Solanki to for cleaning & support\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Urgency Level\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"high\",\n          \"medium\",\n          \"low\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Complaint Keywords\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 695,\n        \"samples\": [\n          \"mob, snatched, in, moving, train, trn, 12002, e2, 4, pnr, no, one, helping, no, fir, frustrating, plz, help\",\n          \"traveling, by, 12005, ndl, shatabdi, pathetic, condition, no, water, in, toilets, train, runing, late, apparently, without, any, reason\",\n          \"what, is, the, actual, price, of, dinner, food, in, railway, i, am, in, motihari, exp, 19269\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Drop rows with missing values in required columns\n",
        "df = df.dropna(subset=[\"Grievance Description\", \"Category\", \"Urgency Level\"])\n",
        "\n",
        "# Encode Category\n",
        "category_encoder = LabelEncoder()\n",
        "df[\"Category_Encoded\"] = category_encoder.fit_transform(df[\"Category\"])\n",
        "\n",
        "# Encode Urgency Level\n",
        "urgency_encoder = LabelEncoder()\n",
        "df[\"Urgency_Encoded\"] = urgency_encoder.fit_transform(df[\"Urgency Level\"])\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "tfidf = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n",
        "X_text = tfidf.fit_transform(df[\"Grievance Description\"])\n",
        "\n",
        "print(\"Preprocessing Done!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49WlF51YGqeN",
        "outputId": "4f7b06b9-045d-4aad-a41f-ebbcd8e5c0b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "category_urgency_counts = df.groupby([\"Category\", \"Urgency Level\"]).size().unstack(fill_value=0)\n",
        "print(category_urgency_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "godlBq0xHmOv",
        "outputId": "3ca567e5-8c50-44ff-d628-e1489a20d4ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Urgency Level       high  low  medium\n",
            "Category                             \n",
            "catering              21   49      45\n",
            "customer service      10   63      48\n",
            "housekeeping          63   17      75\n",
            "luggage                7    4      15\n",
            "maintenance           44   20      60\n",
            "medical and health    26    2       7\n",
            "security              19    4      18\n",
            "train operations      19   37      30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify unexpected urgency levels\n",
        "unexpected_urgency = df[~df[\"Urgency Level\"].isin([\"high\", \"medium\", \"low\"])]\n",
        "\n",
        "# Display the row(s) with unexpected urgency levels\n",
        "print(unexpected_urgency)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuJURtk-HqTm",
        "outputId": "a54e96ee-42f9-4c24-92a8-c3cd9be10134"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: [Grievance ID, Category, Grievance Description, Customer Feedback, Urgency Level, Complaint Keywords, Category_Encoded, Urgency_Encoded]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_val_score\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, f1_score\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import SMOTE, ADASYN\n",
        "from imblearn.combine import SMOTETomek, SMOTEENN\n",
        "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SelectFromModel, RFE, SelectKBest, chi2, f_classif\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "92rSGeqeHuJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Download NLTK resources if not already downloaded\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('punkt_tab')\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lzep8-gRIYou",
        "outputId": "4bf22f42-78a8-4115-b451-9de07d87c748"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Text preprocessing function\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove special characters and punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# Load data\n",
        "print(\"Loading and preprocessing data...\")\n",
        "df = pd.read_csv(\"proc_ds.csv\", encoding=\"ISO-8859-1\")\n",
        "df = df.dropna(subset=['Grievance Description', 'Urgency Level'])\n",
        "\n",
        "# Map urgency levels to numerical values\n",
        "df[\"Urgency Level\"] = df[\"Urgency Level\"].map({\"low\": 0, \"medium\": 1, \"high\": 2})\n",
        "df = df.dropna(subset=['Urgency Level'])\n",
        "\n",
        "# Display class distribution\n",
        "print(\"Class distribution before sampling:\")\n",
        "print(df[\"Urgency Level\"].value_counts(normalize=True).sort_index() * 100)\n",
        "\n",
        "# Preprocess text data\n",
        "df[\"Processed Description\"] = df[\"Grievance Description\"].apply(preprocess_text)\n",
        "\n",
        "# Feature engineering\n",
        "print(\"Extracting features...\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hDzevhjIfqC",
        "outputId": "ada72edd-d56a-401f-947c-f3e3f3e7dc7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preprocessing data...\n",
            "Class distribution before sampling:\n",
            "Urgency Level\n",
            "0    27.880512\n",
            "1    42.389758\n",
            "2    29.729730\n",
            "Name: proportion, dtype: float64\n",
            "Extracting features...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF features with optimized parameters\n",
        "tfidf_vect = TfidfVectorizer(\n",
        "    stop_words=\"english\",\n",
        "    ngram_range=(1, 3),\n",
        "    max_features=10000,\n",
        "    min_df=2,\n",
        "    max_df=0.9,\n",
        "    use_idf=True,\n",
        "    sublinear_tf=True  # Apply sublinear tf scaling (logarithmic)\n",
        ")\n",
        "X_tfidf = tfidf_vect.fit_transform(df[\"Processed Description\"])"
      ],
      "metadata": {
        "id": "Baec4GK1IjXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Count vectorizer with different parameters for additional perspective\n",
        "count_vect = CountVectorizer(\n",
        "    stop_words=\"english\",\n",
        "    ngram_range=(1, 2),\n",
        "    max_features=8000,\n",
        "    min_df=2,\n",
        "    max_df=0.9\n",
        ")\n",
        "X_count = count_vect.fit_transform(df[\"Processed Description\"])"
      ],
      "metadata": {
        "id": "b8uRsQ0WIsaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Extract text-based features\n",
        "text_features = np.array([\n",
        "    [\n",
        "        len(text),                                       # Length of text\n",
        "        len(text.split()),                               # Word count\n",
        "        sum(1 for c in text if c.isupper()),             # Uppercase count\n",
        "        text.count('!'),                                 # Exclamation count\n",
        "        text.count('?'),                                 # Question mark count\n",
        "        sum(1 for c in text if c.isdigit()),             # Digit count\n",
        "        len(re.findall(r'\\b(?:urgent|immediate|critical|emergency|asap|quickly|serious)\\b', text.lower()))  # Urgency terms\n",
        "    ]\n",
        "    for text in df[\"Grievance Description\"]\n",
        "])\n",
        "\n",
        "# Normalize text features\n",
        "scaler = StandardScaler()\n",
        "text_features_scaled = scaler.fit_transform(text_features)"
      ],
      "metadata": {
        "id": "BJ2t01L6IugQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Combine all features\n",
        "X_tfidf_array = X_tfidf.toarray()\n",
        "X_count_array = X_count.toarray()\n",
        "X = np.hstack((X_tfidf_array, X_count_array, text_features_scaled))\n",
        "y = df[\"Urgency Level\"].values\n",
        "\n",
        "# Feature selection to reduce dimensionality and improve performance\n",
        "print(\"Performing feature selection...\")\n",
        "selector = SelectFromModel(\n",
        "    RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    threshold='median'\n",
        ")\n",
        "X = selector.fit_transform(X, y)\n",
        "print(f\"Features reduced from {X_tfidf_array.shape[1] + X_count_array.shape[1] + text_features_scaled.shape[1]} to {X.shape[1]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuZKdauMIwl5",
        "outputId": "28a6dfc4-ca10-43ed-9811-4b2a122c0484"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing feature selection...\n",
            "Features reduced from 3722 to 1861\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# Explore different sampling techniques\n",
        "print(\"Applying advanced resampling...\")\n",
        "# Try different samplers and choose the best\n",
        "samplers = {\n",
        "    'SMOTE': SMOTE(random_state=42),\n",
        "    'ADASYN': ADASYN(random_state=42),\n",
        "    'SMOTETomek': SMOTETomek(random_state=42),\n",
        "    'SMOTEENN': SMOTEENN(random_state=42)\n",
        "}\n",
        "\n",
        "best_sampler = None\n",
        "best_score = 0\n",
        "\n",
        "for name, sampler in samplers.items():\n",
        "    print(f\"Testing {name}...\")\n",
        "    X_resampled, y_resampled = sampler.fit_resample(X_train, y_train)\n",
        "\n",
        "    # Quick evaluation with Random Forest\n",
        "    cv_scores = cross_val_score(\n",
        "        RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "        X_resampled, y_resampled,\n",
        "        cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=42),\n",
        "        scoring='f1_macro'\n",
        "    )\n",
        "\n",
        "    mean_score = cv_scores.mean()\n",
        "    print(f\"{name} cross-validation f1-macro: {mean_score:.4f}\")\n",
        "\n",
        "    if mean_score > best_score:\n",
        "        best_score = mean_score\n",
        "        best_sampler = name\n",
        "\n",
        "print(f\"Selected best sampler: {best_sampler}\")\n",
        "X_train, y_train = samplers[best_sampler].fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"Class distribution after resampling:\")\n",
        "print(pd.Series(y_train).value_counts(normalize=True).sort_index() * 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10APq8u1IyMA",
        "outputId": "ddc7db3d-937a-4fef-ede5-eb87b30ca70a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applying advanced resampling...\n",
            "Testing SMOTE...\n",
            "SMOTE cross-validation f1-macro: 0.6419\n",
            "Testing ADASYN...\n",
            "ADASYN cross-validation f1-macro: 0.6152\n",
            "Testing SMOTETomek...\n",
            "SMOTETomek cross-validation f1-macro: 0.6492\n",
            "Testing SMOTEENN...\n",
            "SMOTEENN cross-validation f1-macro: 0.7798\n",
            "Selected best sampler: SMOTEENN\n",
            "Class distribution after resampling:\n",
            "0    47.867299\n",
            "1    13.744076\n",
            "2    38.388626\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model definition and tuning\n",
        "print(\"Training models...\")\n",
        "\n",
        "# Define models with improved parameters\n",
        "xgb = XGBClassifier(\n",
        "    objective='multi:softprob',\n",
        "    num_class=3,\n",
        "    learning_rate=0.03,\n",
        "    n_estimators=500,\n",
        "    max_depth=7,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    min_child_weight=3,\n",
        "    gamma=0.1,\n",
        "    reg_alpha=0.1,\n",
        "    reg_lambda=1,\n",
        "    scale_pos_weight=1,\n",
        "    random_state=42,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='mlogloss'\n",
        ")\n",
        "\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=500,\n",
        "    max_depth=12,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=2,\n",
        "    max_features='sqrt',\n",
        "    bootstrap=True,\n",
        "    class_weight='balanced',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "gb = GradientBoostingClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "svc = SVC(\n",
        "    C=10.0,\n",
        "    kernel='rbf',\n",
        "    gamma='scale',\n",
        "    probability=True,\n",
        "    class_weight='balanced',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "mlp = MLPClassifier(\n",
        "    hidden_layer_sizes=(100, 50),\n",
        "    activation='relu',\n",
        "    solver='adam',\n",
        "    alpha=0.0001,\n",
        "    batch_size='auto',\n",
        "    learning_rate='adaptive',\n",
        "    max_iter=300,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Stack of base models with meta-learner for better ensemble\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "\n",
        "# Train base models\n",
        "base_models = {\n",
        "    'xgb': xgb,\n",
        "    'rf': rf,\n",
        "    'gb': gb,\n",
        "    'svc': svc,\n",
        "    'mlp': mlp\n",
        "}\n",
        "\n",
        "print(\"Training base models individually...\")\n",
        "base_predictions = {}\n",
        "base_models_trained = {}\n",
        "\n",
        "for name, model in base_models.items():\n",
        "    print(f\"Training {name}...\")\n",
        "    model.fit(X_train, y_train)\n",
        "    base_models_trained[name] = model\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred, average='macro')\n",
        "    base_predictions[name] = y_pred\n",
        "    print(f\"{name} - Accuracy: {accuracy:.4f}, F1 Macro: {f1:.4f}\")\n",
        "    print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7tn6z75I2Yq",
        "outputId": "99cf94fc-bb2a-496f-e384-ff2ee8843190"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training models...\n",
            "Training base models individually...\n",
            "Training xgb...\n",
            "xgb - Accuracy: 0.4539, F1 Macro: 0.4479\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.43      0.59      0.50        39\n",
            "           1       0.56      0.23      0.33        60\n",
            "           2       0.43      0.64      0.51        42\n",
            "\n",
            "    accuracy                           0.45       141\n",
            "   macro avg       0.47      0.49      0.45       141\n",
            "weighted avg       0.49      0.45      0.43       141\n",
            "\n",
            "Training rf...\n",
            "rf - Accuracy: 0.4752, F1 Macro: 0.4784\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.46      0.62      0.53        39\n",
            "           1       0.49      0.35      0.41        60\n",
            "           2       0.48      0.52      0.50        42\n",
            "\n",
            "    accuracy                           0.48       141\n",
            "   macro avg       0.48      0.50      0.48       141\n",
            "weighted avg       0.48      0.48      0.47       141\n",
            "\n",
            "Training gb...\n",
            "gb - Accuracy: 0.4468, F1 Macro: 0.4438\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.42      0.59      0.49        39\n",
            "           1       0.52      0.25      0.34        60\n",
            "           2       0.44      0.60      0.51        42\n",
            "\n",
            "    accuracy                           0.45       141\n",
            "   macro avg       0.46      0.48      0.44       141\n",
            "weighted avg       0.47      0.45      0.43       141\n",
            "\n",
            "Training svc...\n",
            "svc - Accuracy: 0.4965, F1 Macro: 0.4905\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.45      0.59      0.51        39\n",
            "           1       0.81      0.28      0.42        60\n",
            "           2       0.43      0.71      0.54        42\n",
            "\n",
            "    accuracy                           0.50       141\n",
            "   macro avg       0.57      0.53      0.49       141\n",
            "weighted avg       0.60      0.50      0.48       141\n",
            "\n",
            "Training mlp...\n",
            "mlp - Accuracy: 0.4326, F1 Macro: 0.4247\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.45      0.56      0.50        39\n",
            "           1       0.57      0.20      0.30        60\n",
            "           2       0.38      0.64      0.48        42\n",
            "\n",
            "    accuracy                           0.43       141\n",
            "   macro avg       0.47      0.47      0.42       141\n",
            "weighted avg       0.48      0.43      0.41       141\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a stacked ensemble model\n",
        "print(\"Training stacked ensemble...\")\n",
        "stacking_model = StackingClassifier(\n",
        "    estimators=[\n",
        "        ('xgb', xgb),\n",
        "        ('rf', rf),\n",
        "        ('gb', gb),\n",
        "        ('svc', svc),\n",
        "        ('mlp', mlp)\n",
        "    ],\n",
        "    final_estimator=XGBClassifier(\n",
        "        objective='multi:softprob',\n",
        "        num_class=3,\n",
        "        learning_rate=0.03,\n",
        "        n_estimators=200,\n",
        "        random_state=42\n",
        "    ),\n",
        "    cv=5,\n",
        "    stack_method='predict_proba',\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "stacking_model.fit(X_train, y_train)\n",
        "stack_pred = stacking_model.predict(X_test)\n",
        "stack_accuracy = accuracy_score(y_test, stack_pred)\n",
        "stack_f1 = f1_score(y_test, stack_pred, average='macro')\n",
        "\n",
        "print(\"\\nStacked Ensemble Results:\")\n",
        "print(f\"Accuracy: {stack_accuracy:.4f}, F1 Macro: {stack_f1:.4f}\")\n",
        "print(classification_report(y_test, stack_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVVJQ6ktI_Ty",
        "outputId": "1efbddc2-7ffb-42df-c382-3b6968a169e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training stacked ensemble...\n",
            "\n",
            "Stacked Ensemble Results:\n",
            "Accuracy: 0.4681, F1 Macro: 0.4686\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.49      0.46      0.47        39\n",
            "           1       0.55      0.40      0.46        60\n",
            "           2       0.40      0.57      0.47        42\n",
            "\n",
            "    accuracy                           0.47       141\n",
            "   macro avg       0.48      0.48      0.47       141\n",
            "weighted avg       0.49      0.47      0.47       141\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create voting ensemble\n",
        "print(\"Training voting ensemble...\")\n",
        "voting_model = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('xgb', base_models_trained['xgb']),\n",
        "        ('rf', base_models_trained['rf']),\n",
        "        ('gb', base_models_trained['gb']),\n",
        "        ('svc', base_models_trained['svc']),\n",
        "        ('mlp', base_models_trained['mlp'])\n",
        "    ],\n",
        "    voting='soft',  # Use probability-based voting\n",
        "    weights=[3, 2, 2, 1, 1]  # Weight models by their individual performance\n",
        ")\n",
        "\n",
        "voting_model.fit(X_train, y_train)\n",
        "voting_pred = voting_model.predict(X_test)\n",
        "voting_accuracy = accuracy_score(y_test, voting_pred)\n",
        "voting_f1 = f1_score(y_test, voting_pred, average='macro')\n",
        "\n",
        "print(\"\\nVoting Ensemble Results:\")\n",
        "print(f\"Accuracy: {voting_accuracy:.4f}, F1 Macro: {voting_f1:.4f}\")\n",
        "print(classification_report(y_test, voting_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsR2UIj2JPW4",
        "outputId": "73237ab5-744d-4e27-f94c-566ba7e725d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training voting ensemble...\n",
            "\n",
            "Voting Ensemble Results:\n",
            "Accuracy: 0.4610, F1 Macro: 0.4534\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.42      0.56      0.48        39\n",
            "           1       0.56      0.23      0.33        60\n",
            "           2       0.45      0.69      0.55        42\n",
            "\n",
            "    accuracy                           0.46       141\n",
            "   macro avg       0.48      0.50      0.45       141\n",
            "weighted avg       0.49      0.46      0.44       141\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Advanced ensemble with bias-correction\n",
        "print(\"Training advanced weighted ensemble...\")\n",
        "# Get predictions from each model\n",
        "all_preds = np.array([\n",
        "    base_models_trained['xgb'].predict_proba(X_test),\n",
        "    base_models_trained['rf'].predict_proba(X_test),\n",
        "    base_models_trained['gb'].predict_proba(X_test),\n",
        "    base_models_trained['svc'].predict_proba(X_test),\n",
        "    base_models_trained['mlp'].predict_proba(X_test)\n",
        "])\n",
        "\n",
        "# Use weighted average for final predictions\n",
        "# Weights are dynamically adjusted based on individual model performances\n",
        "individual_accuracies = [\n",
        "    accuracy_score(y_test, base_predictions['xgb']),\n",
        "    accuracy_score(y_test, base_predictions['rf']),\n",
        "    accuracy_score(y_test, base_predictions['gb']),\n",
        "    accuracy_score(y_test, base_predictions['svc']),\n",
        "    accuracy_score(y_test, base_predictions['mlp'])\n",
        "]\n",
        "# Normalize weights\n",
        "weights = np.array(individual_accuracies) / sum(individual_accuracies)\n",
        "print(\"Model weights:\", weights)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NT7te_FhJpdr",
        "outputId": "1364ac40-867a-4acf-ff0b-59ff730b245a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training advanced weighted ensemble...\n",
            "Model weights: [0.19692308 0.20615385 0.19384615 0.21538462 0.18769231]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Calculate weighted average probabilities\n",
        "weighted_probs = sum(weights[i] * all_preds[i] for i in range(len(all_preds)))\n",
        "advanced_ensemble_pred = np.argmax(weighted_probs, axis=1)\n",
        "\n",
        "advanced_accuracy = accuracy_score(y_test, advanced_ensemble_pred)\n",
        "advanced_f1 = f1_score(y_test, advanced_ensemble_pred, average='macro')\n",
        "\n",
        "print(\"\\nAdvanced Weighted Ensemble Results:\")\n",
        "print(f\"Accuracy: {advanced_accuracy:.4f}, F1 Macro: {advanced_f1:.4f}\")\n",
        "\n",
        "print(classification_report(y_test, advanced_ensemble_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cU2io76gJxX9",
        "outputId": "b2f0e405-59ca-4177-e280-94199ef3b8f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Advanced Weighted Ensemble Results:\n",
            "Accuracy: 0.4539, F1 Macro: 0.4460\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.41      0.54      0.47        39\n",
            "           1       0.56      0.23      0.33        60\n",
            "           2       0.45      0.69      0.54        42\n",
            "\n",
            "    accuracy                           0.45       141\n",
            "   macro avg       0.47      0.49      0.45       141\n",
            "weighted avg       0.49      0.45      0.43       141\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize confusion matrices\n",
        "models = {\n",
        "    'XGBoost': base_predictions['xgb'],\n",
        "    'Random Forest': base_predictions['rf'],\n",
        "    'Stacked Ensemble': stack_pred,\n",
        "    'Voting Ensemble': voting_pred,\n",
        "    'Advanced Weighted Ensemble': advanced_ensemble_pred\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(20, 15))\n",
        "for i, (name, preds) in enumerate(models.items(), 1):\n",
        "    plt.subplot(2, 3, i)\n",
        "    cm = confusion_matrix(y_test, preds)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Low', 'Medium', 'High'],\n",
        "                yticklabels=['Low', 'Medium', 'High'])\n",
        "    plt.title(f'{name} Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('confusion_matrices.png')\n",
        "plt.close()\n",
        "\n",
        "# Compare all results\n",
        "print(\"\\nFinal Accuracy Comparison:\")\n",
        "results = {\n",
        "    'XGBoost': accuracy_score(y_test, base_predictions['xgb']),\n",
        "    'Random Forest': accuracy_score(y_test, base_predictions['rf']),\n",
        "    'Gradient Boosting': accuracy_score(y_test, base_predictions['gb']),\n",
        "    'SVC': accuracy_score(y_test, base_predictions['svc']),\n",
        "    'MLP': accuracy_score(y_test, base_predictions['mlp']),\n",
        "    'Stacked Ensemble': stack_accuracy,\n",
        "    'Voting Ensemble': voting_accuracy,\n",
        "    'Advanced Weighted Ensemble': advanced_accuracy\n",
        "}\n",
        "\n",
        "for model, acc in sorted(results.items(), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"{model}: {acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iin9TSnVJz4q",
        "outputId": "b14b0eef-e14f-4887-d5b5-93124df60340"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Accuracy Comparison:\n",
            "SVC: 0.4965\n",
            "Random Forest: 0.4752\n",
            "Stacked Ensemble: 0.4681\n",
            "Voting Ensemble: 0.4610\n",
            "XGBoost: 0.4539\n",
            "Advanced Weighted Ensemble: 0.4539\n",
            "Gradient Boosting: 0.4468\n",
            "MLP: 0.4326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "df = pd.read_csv(\"proc_ds.csv\", encoding=\"ISO-8859-1\")\n",
        "df = df.dropna(subset=['Grievance Description', 'Urgency Level'])\n",
        "df[\"Urgency Level\"] = df[\"Urgency Level\"].str.lower()\n",
        "df = df[df[\"Urgency Level\"].isin([\"low\", \"medium\", \"high\"])]\n",
        "urgency_mapping = {\"low\": 0, \"medium\": 1, \"high\": 2}\n",
        "df[\"Urgency Level\"] = df[\"Urgency Level\"].map(urgency_mapping)\n",
        "tfidf_vect = TfidfVectorizer(stop_words=\"english\", ngram_range=(1, 2), max_features=5000)\n",
        "X = tfidf_vect.fit_transform(df[\"Grievance Description\"]).toarray()\n",
        "y = df[\"Urgency Level\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "base_models = [\n",
        "    ('svc', SVC(kernel='linear', probability=True, C=1.0)),\n",
        "    ('xgb', XGBClassifier(n_estimators=200, max_depth=5, learning_rate=0.05, use_label_encoder=False, eval_metric='mlogloss')),\n",
        "    ('mlp', MLPClassifier(hidden_layer_sizes=(50,), activation='relu', solver='adam', max_iter=500))\n",
        "]\n",
        "\n",
        "meta_learner = LogisticRegression()\n",
        "stacked_model = StackingClassifier(estimators=base_models, final_estimator=meta_learner, passthrough=True)\n",
        "stacked_model.fit(X_train, y_train)\n",
        "y_pred = stacked_model.predict(X_test)\n",
        "\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdomWJzoJ2iX",
        "outputId": "51d92646-3686-4509-97da-3096ff059e9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.4752\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.33      0.41        39\n",
            "           1       0.44      0.65      0.52        60\n",
            "           2       0.54      0.36      0.43        42\n",
            "\n",
            "    accuracy                           0.48       141\n",
            "   macro avg       0.51      0.45      0.45       141\n",
            "weighted avg       0.50      0.48      0.46       141\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier, GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ue4U7BgbKlQ",
        "outputId": "5f8481a5-3dfe-4eed-fab7-4a52bf8a01a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Text preprocessing function\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove special characters and digits\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Lemmatize\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    # Join tokens back into text\n",
        "    processed_text = ' '.join(tokens)\n",
        "\n",
        "    return processed_text\n",
        "\n",
        "# Load and preprocess the data\n",
        "df = pd.read_csv(\"proc_ds.csv\", encoding=\"ISO-8859-1\")\n",
        "df = df.dropna(subset=['Grievance Description', 'Urgency Level'])\n",
        "df[\"Urgency Level\"] = df[\"Urgency Level\"].str.lower()\n",
        "df = df[df[\"Urgency Level\"].isin([\"low\", \"medium\", \"high\"])]\n",
        "\n",
        "# Add Category as a feature if it's available in your dataset\n",
        "has_category = 'Category' in df.columns\n",
        "\n",
        "# Preprocess the text\n",
        "df[\"Processed_Description\"] = df[\"Grievance Description\"].apply(preprocess_text)\n",
        "\n",
        "# Extract additional features\n",
        "df['Description_Length'] = df['Grievance Description'].apply(len)\n",
        "df['Word_Count'] = df['Grievance Description'].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "# Map urgency levels to numerical values\n",
        "urgency_mapping = {\"low\": 0, \"medium\": 1, \"high\": 2}\n",
        "df[\"Urgency Level\"] = df[\"Urgency Level\"].map(urgency_mapping)\n",
        "\n",
        "# TF-IDF transformation with improved parameters\n",
        "tfidf_vect = TfidfVectorizer(\n",
        "    stop_words=\"english\",\n",
        "    ngram_range=(1, 3),  # Include up to trigrams\n",
        "    max_features=10000,  # Increase features\n",
        "    min_df=2,           # Minimum document frequency\n",
        "    max_df=0.9,         # Maximum document frequency\n",
        "    sublinear_tf=True   # Apply sublinear tf scaling\n",
        ")\n",
        "\n",
        "# Prepare feature set\n",
        "X_text = tfidf_vect.fit_transform(df[\"Processed_Description\"])\n",
        "additional_features = df[['Description_Length', 'Word_Count']].values\n",
        "\n",
        "# Add Category as one-hot encoded features if available\n",
        "if has_category:\n",
        "    category_dummies = pd.get_dummies(df['Category'], prefix='category')\n",
        "    additional_features = np.hstack((additional_features, category_dummies.values))\n",
        "\n",
        "# Combine text features and additional features\n",
        "X = np.hstack((X_text.toarray(), additional_features))\n",
        "y = df[\"Urgency Level\"]\n",
        "\n",
        "# Split the data with stratification\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# Apply SMOTE to handle class imbalance\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Set up base models with optimized hyperparameters\n",
        "base_models = [\n",
        "    ('svc', SVC(\n",
        "        kernel='rbf',\n",
        "        C=10.0,\n",
        "        gamma='scale',\n",
        "        probability=True,\n",
        "        class_weight='balanced'\n",
        "    )),\n",
        "    ('xgb', XGBClassifier(\n",
        "        n_estimators=300,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.01,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        min_child_weight=3,\n",
        "        eval_metric='mlogloss',\n",
        "        use_label_encoder=False\n",
        "    )),\n",
        "    ('rf', RandomForestClassifier(\n",
        "        n_estimators=200,\n",
        "        max_depth=15,\n",
        "        min_samples_split=5,\n",
        "        min_samples_leaf=2,\n",
        "        bootstrap=True,\n",
        "        class_weight='balanced'\n",
        "    )),\n",
        "    ('gb', GradientBoostingClassifier(\n",
        "        n_estimators=200,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=5,\n",
        "        min_samples_split=4,\n",
        "        min_samples_leaf=2,\n",
        "        subsample=0.8\n",
        "    )),\n",
        "    ('mlp', MLPClassifier(\n",
        "        hidden_layer_sizes=(100, 50),\n",
        "        activation='relu',\n",
        "        solver='adam',\n",
        "        alpha=0.0001,\n",
        "        batch_size=64,\n",
        "        learning_rate='adaptive',\n",
        "        max_iter=1000,\n",
        "        early_stopping=True\n",
        "    ))\n",
        "]\n",
        "\n",
        "# Set up meta learner with cross-validation\n",
        "meta_learner = LogisticRegression(\n",
        "    C=1.0,\n",
        "    solver='lbfgs',\n",
        "    max_iter=1000,\n",
        "    class_weight='balanced'\n",
        ")\n",
        "\n",
        "# Create and train the stacked model with cross-validation\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "stacked_model = StackingClassifier(\n",
        "    estimators=base_models,\n",
        "    final_estimator=meta_learner,\n",
        "    cv=cv,\n",
        "    passthrough=True\n",
        ")\n",
        "\n",
        "# Fit the stacked model on the resampled training data\n",
        "stacked_model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Predict and evaluate on the test set\n",
        "y_pred = stacked_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "'''\n",
        "# Function to predict urgency for new grievances\n",
        "def predict_urgency(grievance_text, model=stacked_model):\n",
        "    # Preprocess the text\n",
        "    processed_text = preprocess_text(grievance_text)\n",
        "\n",
        "    # Transform with TF-IDF\n",
        "    text_features = tfidf_vect.transform([processed_text]).toarray()\n",
        "\n",
        "    # Calculate additional features\n",
        "    desc_length = len(grievance_text)\n",
        "    word_count = len(grievance_text.split())\n",
        "    add_features = np.array([[desc_length, word_count]])\n",
        "\n",
        "    # Combine features\n",
        "    features = np.hstack((text_features, add_features))\n",
        "\n",
        "    # If category features were used, add placeholder zeros\n",
        "    if has_category and additional_features.shape[1] > 2:\n",
        "        category_zeros = np.zeros((1, additional_features.shape[1] - 2))\n",
        "        features = np.hstack((features, category_zeros))\n",
        "\n",
        "    # Predict\n",
        "    urgency_num = model.predict(features)[0]\n",
        "\n",
        "    # Map back to labels\n",
        "    urgency_labels = {0: \"low\", 1: \"medium\", 2: \"high\"}\n",
        "    return urgency_labels[urgency_num]'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "-5GlJKyxKHON",
        "outputId": "a2898785-fab4-4c79-b294-0eac83099796"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.4894\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.44      0.52        39\n",
            "           1       0.45      0.62      0.52        60\n",
            "           2       0.47      0.36      0.41        42\n",
            "\n",
            "    accuracy                           0.49       141\n",
            "   macro avg       0.52      0.47      0.48       141\n",
            "weighted avg       0.51      0.49      0.49       141\n",
            "\n",
            "Confusion Matrix:\n",
            " [[17 20  2]\n",
            " [ 8 37 15]\n",
            " [ 2 25 15]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Function to predict urgency for new grievances\\ndef predict_urgency(grievance_text, model=stacked_model):\\n    # Preprocess the text\\n    processed_text = preprocess_text(grievance_text)\\n    \\n    # Transform with TF-IDF\\n    text_features = tfidf_vect.transform([processed_text]).toarray()\\n    \\n    # Calculate additional features\\n    desc_length = len(grievance_text)\\n    word_count = len(grievance_text.split())\\n    add_features = np.array([[desc_length, word_count]])\\n    \\n    # Combine features\\n    features = np.hstack((text_features, add_features))\\n    \\n    # If category features were used, add placeholder zeros\\n    if has_category and additional_features.shape[1] > 2:\\n        category_zeros = np.zeros((1, additional_features.shape[1] - 2))\\n        features = np.hstack((features, category_zeros))\\n    \\n    # Predict\\n    urgency_num = model.predict(features)[0]\\n    \\n    # Map back to labels\\n    urgency_labels = {0: \"low\", 1: \"medium\", 2: \"high\"}\\n    return urgency_labels[urgency_num]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "# Custom text preprocessing with memory efficiency in mind\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Convert to lowercase and remove non-alphabetic characters\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text.lower())\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords and short words (often not meaningful)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n",
        "\n",
        "    # Stemming (less resource-intensive than lemmatization)\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Feature extraction functions\n",
        "def extract_features(df):\n",
        "    # Add important text metadata features\n",
        "    features = pd.DataFrame()\n",
        "\n",
        "    # Text length features\n",
        "    features['text_length'] = df['Grievance Description'].apply(lambda x: len(str(x)))\n",
        "\n",
        "    # Word count\n",
        "    features['word_count'] = df['Grievance Description'].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "    # Exclamation mark count (might indicate urgency)\n",
        "    features['exclamation_count'] = df['Grievance Description'].apply(lambda x: str(x).count('!'))\n",
        "\n",
        "    # Question mark count\n",
        "    features['question_count'] = df['Grievance Description'].apply(lambda x: str(x).count('?'))\n",
        "\n",
        "    # Uppercase word count (might indicate emphasis/urgency)\n",
        "    features['uppercase_word_count'] = df['Grievance Description'].apply(\n",
        "        lambda x: sum(1 for word in str(x).split() if word.isupper() and len(word) > 1)\n",
        "    )\n",
        "\n",
        "    # Category as feature if available\n",
        "    if 'Category' in df.columns:\n",
        "        features['Category'] = df['Category']\n",
        "\n",
        "    return features\n",
        "\n",
        "# Load and prepare the data with memory efficiency in mind\n",
        "print(\"Loading and preprocessing data...\")\n",
        "df = pd.read_csv(\"proc_ds.csv\", encoding=\"ISO-8859-1\")\n",
        "df = df.dropna(subset=['Grievance Description', 'Urgency Level'])\n",
        "df[\"Urgency Level\"] = df[\"Urgency Level\"].str.lower()\n",
        "df = df[df[\"Urgency Level\"].isin([\"low\", \"medium\", \"high\"])]\n",
        "\n",
        "# Map urgency levels to numerical values\n",
        "urgency_mapping = {\"low\": 0, \"medium\": 1, \"high\": 2}\n",
        "df[\"Urgency Level\"] = df[\"Urgency Level\"].map(urgency_mapping)\n",
        "\n",
        "# Print class distribution\n",
        "print(\"Class distribution before sampling:\")\n",
        "print(df[\"Urgency Level\"].value_counts())\n",
        "\n",
        "# Extract additional features\n",
        "print(\"Extracting features...\")\n",
        "additional_features = extract_features(df)\n",
        "\n",
        "# Apply text preprocessing\n",
        "print(\"Preprocessing text...\")\n",
        "df['processed_text'] = df['Grievance Description'].apply(preprocess_text)\n",
        "\n",
        "# Create category features if available\n",
        "categorical_features = []\n",
        "if 'Category' in additional_features.columns:\n",
        "    categorical_features = ['Category']\n",
        "    print(f\"Using categories: {df['Category'].unique()}\")\n",
        "\n",
        "numeric_features = [col for col in additional_features.columns if col != 'Category']\n",
        "\n",
        "# Use smaller TF-IDF parameters to reduce memory usage\n",
        "print(\"Creating TF-IDF features...\")\n",
        "tfidf = TfidfVectorizer(\n",
        "    max_features=2000,  # Reduced feature count\n",
        "    min_df=2,\n",
        "    max_df=0.85,\n",
        "    ngram_range=(1, 2),\n",
        "    sublinear_tf=True,\n",
        "    use_idf=True,\n",
        "    norm='l2'\n",
        ")\n",
        "\n",
        "# Split data with stratification\n",
        "X_text = df['processed_text']\n",
        "X_features = additional_features\n",
        "y = df[\"Urgency Level\"]\n",
        "\n",
        "# Create preprocessing pipelines\n",
        "print(\"Building model pipeline...\")\n",
        "text_pipeline = Pipeline([\n",
        "    ('tfidf', tfidf)\n",
        "])\n",
        "\n",
        "# Create column transformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('text', text_pipeline, 'processed_text'),\n",
        "        ('num', StandardScaler(), numeric_features)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "if categorical_features:\n",
        "    # Add categorical features\n",
        "    cat_preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "        ],\n",
        "        remainder='passthrough'\n",
        "    )\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('preprocess', preprocessor, ['processed_text'] + numeric_features),\n",
        "            ('cat_encode', cat_preprocessor, categorical_features)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "# Use a memory-efficient model - Random Forest with limited depth and trees\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=150,\n",
        "    max_depth=10,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=2,\n",
        "    class_weight='balanced',\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Create a pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', rf_model)\n",
        "])\n",
        "\n",
        "# Prepare data for pipeline\n",
        "X_combined = pd.concat([\n",
        "    X_text.reset_index(drop=True),\n",
        "    X_features.reset_index(drop=True)\n",
        "], axis=1)\n",
        "\n",
        "# Stratified sampling to handle class imbalance\n",
        "# Undersample the majority classes instead of oversampling to save memory\n",
        "class_counts = df[\"Urgency Level\"].value_counts()\n",
        "min_class_count = class_counts.min()\n",
        "balanced_df = pd.DataFrame()\n",
        "\n",
        "for class_val in df[\"Urgency Level\"].unique():\n",
        "    class_df = df[df[\"Urgency Level\"] == class_val]\n",
        "    if len(class_df) > min_class_count:\n",
        "        # Undersample\n",
        "        class_df = class_df.sample(min_class_count, random_state=42)\n",
        "    balanced_df = pd.concat([balanced_df, class_df])\n",
        "\n",
        "# Recreate features and text after balancing\n",
        "balanced_X_text = balanced_df['processed_text']\n",
        "balanced_X_features = extract_features(balanced_df)\n",
        "balanced_y = balanced_df[\"Urgency Level\"]\n",
        "\n",
        "balanced_X_combined = pd.concat([\n",
        "    balanced_X_text.reset_index(drop=True),\n",
        "    balanced_X_features.reset_index(drop=True)\n",
        "], axis=1)\n",
        "\n",
        "# Verify class balance\n",
        "print(\"Class distribution after balancing:\")\n",
        "print(balanced_y.value_counts())\n",
        "\n",
        "# Train-test split with stratification\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    balanced_X_combined, balanced_y,\n",
        "    test_size=0.2,\n",
        "    stratify=balanced_y,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "print(\"Training model...\")\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Evaluating model...\")\n",
        "# Get cross-validation score first\n",
        "cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='accuracy')\n",
        "print(f\"Cross-validation accuracy: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = pipeline.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test accuracy: {accuracy:.4f}\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Feature importance analysis\n",
        "if hasattr(pipeline.named_steps['classifier'], 'feature_importances_'):\n",
        "    importances = pipeline.named_steps['classifier'].feature_importances_\n",
        "    print(\"\\nTop important features:\")\n",
        "    if hasattr(pipeline.named_steps['preprocessor'], 'get_feature_names_out'):\n",
        "        feature_names = pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
        "        importance_df = pd.DataFrame({\n",
        "            'feature': feature_names,\n",
        "            'importance': importances\n",
        "        }).sort_values('importance', ascending=False)\n",
        "        print(importance_df.head(20))\n",
        "'''\n",
        "# Simple function to predict on new data\n",
        "def predict_urgency(text, category=None):\n",
        "    # Create a DataFrame with the same structure as training data\n",
        "    data = pd.DataFrame({\n",
        "        'processed_text': [preprocess_text(text)],\n",
        "    })\n",
        "\n",
        "    # Add text features\n",
        "    data['text_length'] = [len(text)]\n",
        "    data['word_count'] = [len(text.split())]\n",
        "    data['exclamation_count'] = [text.count('!')]\n",
        "    data['question_count'] = [text.count('?')]\n",
        "    data['uppercase_word_count'] = [sum(1 for word in text.split() if word.isupper() and len(word) > 1)]\n",
        "\n",
        "    # Add category if provided and was used in training\n",
        "    if category is not None and 'Category' in additional_features.columns:\n",
        "        data['Category'] = [category]\n",
        "\n",
        "    # Make prediction\n",
        "    prediction = pipeline.predict(data)[0]\n",
        "    proba = pipeline.predict_proba(data)[0]\n",
        "\n",
        "    # Map back to labels\n",
        "    urgency_labels = {0: \"low\", 1: \"medium\", 2: \"high\"}\n",
        "\n",
        "    return {\n",
        "        'prediction': urgency_labels[prediction],\n",
        "        'confidence': max(proba) * 100,\n",
        "        'probabilities': {urgency_labels[i]: proba[i] * 100 for i in range(len(proba))}\n",
        "    }\n",
        "\n",
        "# Simple sentiment-based validation model to check our results\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "print(\"\\nTraining simple validation model...\")\n",
        "count_vec = CountVectorizer(max_features=1000)\n",
        "X_text_counts = count_vec.fit_transform(balanced_X_text)\n",
        "simple_model = MultinomialNB()\n",
        "simple_model.fit(X_text_counts, balanced_y)\n",
        "\n",
        "# Check validation model accuracy\n",
        "X_test_counts = count_vec.transform(X_test['processed_text'])\n",
        "simple_pred = simple_model.predict(X_test_counts)\n",
        "simple_accuracy = accuracy_score(y_test, simple_pred)\n",
        "print(f\"Simple model accuracy: {simple_accuracy:.4f}\")\n",
        "\n",
        "# Try to determine what type of texts are classified as what urgency level\n",
        "print(\"\\nAnalyzing patterns in urgency levels:\")\n",
        "for level in [0, 1, 2]:\n",
        "    level_texts = df[df[\"Urgency Level\"] == level]['processed_text']\n",
        "    if len(level_texts) > 0:\n",
        "        # Get most common words for this level\n",
        "        level_counts = CountVectorizer(max_features=20).fit_transform(level_texts)\n",
        "        level_words = CountVectorizer(max_features=20).fit(level_texts).get_feature_names_out()\n",
        "        level_word_counts = level_counts.sum(axis=0).tolist()[0]\n",
        "        level_word_freq = sorted(zip(level_words, level_word_counts), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        urgency_labels = {0: \"low\", 1: \"medium\", 2: \"high\"}\n",
        "        print(f\"{urgency_labels[level]} urgency common words: {level_word_freq[:10]}\")\n",
        "\n",
        "# Save model for future use\n",
        "import pickle\n",
        "print(\"\\nSaving model to 'grievance_model.pkl'\")\n",
        "with open('grievance_model.pkl', 'wb') as f:\n",
        "    pickle.dump(pipeline, f)\n",
        "print(\"Model saved successfully\")'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vY_7bKZvbMtK",
        "outputId": "8ee5901d-9a33-4689-ef59-0fe053ba9e82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preprocessing data...\n",
            "Class distribution before sampling:\n",
            "Urgency Level\n",
            "1    298\n",
            "2    209\n",
            "0    196\n",
            "Name: count, dtype: int64\n",
            "Extracting features...\n",
            "Preprocessing text...\n",
            "Using categories: ['medical and health' 'maintenance' 'train operations' 'housekeeping'\n",
            " 'catering' 'security' 'customer service' 'luggage']\n",
            "Creating TF-IDF features...\n",
            "Building model pipeline...\n",
            "Class distribution after balancing:\n",
            "Urgency Level\n",
            "2    196\n",
            "1    196\n",
            "0    196\n",
            "Name: count, dtype: int64\n",
            "Training model...\n",
            "Evaluating model...\n",
            "Cross-validation accuracy: 0.4894 ± 0.0404\n",
            "Test accuracy: 0.5339\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.77      0.65        39\n",
            "           1       0.39      0.28      0.33        39\n",
            "           2       0.61      0.55      0.58        40\n",
            "\n",
            "    accuracy                           0.53       118\n",
            "   macro avg       0.52      0.53      0.52       118\n",
            "weighted avg       0.52      0.53      0.52       118\n",
            "\n",
            "Confusion Matrix:\n",
            " [[30  6  3]\n",
            " [17 11 11]\n",
            " [ 7 11 22]]\n",
            "\n",
            "Top important features:\n",
            "                                           feature  importance\n",
            "135                        preprocess__text__coach    0.030814\n",
            "1059    cat_encode__cat__Category_customer service    0.029123\n",
            "1053                  preprocess__num__text_length    0.027376\n",
            "329                         preprocess__text__help    0.026788\n",
            "1065    cat_encode__cat__Category_train operations    0.024012\n",
            "1060        cat_encode__cat__Category_housekeeping    0.021866\n",
            "1054                   preprocess__num__word_count    0.021343\n",
            "1063  cat_encode__cat__Category_medical and health    0.020024\n",
            "404                         preprocess__text__late    0.019515\n",
            "873                       preprocess__text__tatkal    0.018428\n",
            "891                       preprocess__text__ticket    0.017835\n",
            "916                        preprocess__text__train    0.016541\n",
            "345                           preprocess__text__hr    0.016050\n",
            "663             preprocess__text__railminindia pnr    0.013999\n",
            "496                         preprocess__text__need    0.013370\n",
            "1012                       preprocess__text__water    0.012408\n",
            "852                preprocess__text__sureshpprabhu    0.011712\n",
            "908                       preprocess__text__toilet    0.011486\n",
            "922                  preprocess__text__train coach    0.011127\n",
            "341                         preprocess__text__hour    0.010794\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Simple function to predict on new data\\ndef predict_urgency(text, category=None):\\n    # Create a DataFrame with the same structure as training data\\n    data = pd.DataFrame({\\n        \\'processed_text\\': [preprocess_text(text)],\\n    })\\n    \\n    # Add text features\\n    data[\\'text_length\\'] = [len(text)]\\n    data[\\'word_count\\'] = [len(text.split())]\\n    data[\\'exclamation_count\\'] = [text.count(\\'!\\')]\\n    data[\\'question_count\\'] = [text.count(\\'?\\')]\\n    data[\\'uppercase_word_count\\'] = [sum(1 for word in text.split() if word.isupper() and len(word) > 1)]\\n    \\n    # Add category if provided and was used in training\\n    if category is not None and \\'Category\\' in additional_features.columns:\\n        data[\\'Category\\'] = [category]\\n    \\n    # Make prediction\\n    prediction = pipeline.predict(data)[0]\\n    proba = pipeline.predict_proba(data)[0]\\n    \\n    # Map back to labels\\n    urgency_labels = {0: \"low\", 1: \"medium\", 2: \"high\"}\\n    \\n    return {\\n        \\'prediction\\': urgency_labels[prediction],\\n        \\'confidence\\': max(proba) * 100,\\n        \\'probabilities\\': {urgency_labels[i]: proba[i] * 100 for i in range(len(proba))}\\n    }\\n\\n# Simple sentiment-based validation model to check our results\\nfrom sklearn.feature_extraction.text import CountVectorizer\\nfrom sklearn.naive_bayes import MultinomialNB\\n\\nprint(\"\\nTraining simple validation model...\")\\ncount_vec = CountVectorizer(max_features=1000)\\nX_text_counts = count_vec.fit_transform(balanced_X_text)\\nsimple_model = MultinomialNB()\\nsimple_model.fit(X_text_counts, balanced_y)\\n\\n# Check validation model accuracy\\nX_test_counts = count_vec.transform(X_test[\\'processed_text\\'])\\nsimple_pred = simple_model.predict(X_test_counts)\\nsimple_accuracy = accuracy_score(y_test, simple_pred)\\nprint(f\"Simple model accuracy: {simple_accuracy:.4f}\")\\n\\n# Try to determine what type of texts are classified as what urgency level\\nprint(\"\\nAnalyzing patterns in urgency levels:\")\\nfor level in [0, 1, 2]:\\n    level_texts = df[df[\"Urgency Level\"] == level][\\'processed_text\\']\\n    if len(level_texts) > 0:\\n        # Get most common words for this level\\n        level_counts = CountVectorizer(max_features=20).fit_transform(level_texts)\\n        level_words = CountVectorizer(max_features=20).fit(level_texts).get_feature_names_out()\\n        level_word_counts = level_counts.sum(axis=0).tolist()[0]\\n        level_word_freq = sorted(zip(level_words, level_word_counts), key=lambda x: x[1], reverse=True)\\n        \\n        urgency_labels = {0: \"low\", 1: \"medium\", 2: \"high\"}\\n        print(f\"{urgency_labels[level]} urgency common words: {level_word_freq[:10]}\")\\n\\n# Save model for future use\\nimport pickle\\nprint(\"\\nSaving model to \\'grievance_model.pkl\\'\")\\nwith open(\\'grievance_model.pkl\\', \\'wb\\') as f:\\n    pickle.dump(pipeline, f)\\nprint(\"Model saved successfully\")'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import re\n",
        "import warnings\n",
        "import random\n",
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "print(\"Loading and preparing data...\")\n",
        "df = pd.read_csv(\"proc_ds.csv\", encoding=\"ISO-8859-1\")\n",
        "df = df.dropna(subset=['Grievance Description', 'Urgency Level'])\n",
        "df[\"Urgency Level\"] = df[\"Urgency Level\"].str.lower()\n",
        "df = df[df[\"Urgency Level\"].isin([\"low\", \"medium\", \"high\"])]\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "df[\"urgency_encoded\"] = label_encoder.fit_transform(df[\"Urgency Level\"])\n",
        "\n",
        "print(\"Original class distribution:\")\n",
        "print(df[\"Urgency Level\"].value_counts())\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text.lower())\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = [stemmer.stem(word) for word in tokens]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "print(\"Preprocessing text...\")\n",
        "df['processed_text'] = df['Grievance Description'].apply(preprocess_text)\n",
        "\n",
        "def extract_features(text_series):\n",
        "    features = pd.DataFrame()\n",
        "    features['text_length'] = text_series.apply(len)\n",
        "    features['word_count'] = text_series.apply(lambda x: len(str(x).split()))\n",
        "    features['avg_word_length'] = text_series.apply(\n",
        "        lambda x: np.mean([len(word) for word in str(x).split()]) if len(str(x).split()) > 0 else 0\n",
        "    )\n",
        "    features['unique_word_ratio'] = text_series.apply(\n",
        "        lambda x: len(set(str(x).split())) / (len(str(x).split()) or 1)\n",
        "    )\n",
        "    features['exclamation_count'] = text_series.apply(lambda x: str(x).count('!'))\n",
        "    features['question_count'] = text_series.apply(lambda x: str(x).count('?'))\n",
        "    features['uppercase_ratio'] = text_series.apply(\n",
        "        lambda x: sum(1 for c in str(x) if c.isupper()) / (len(str(x)) or 1)\n",
        "    )\n",
        "    features['number_count'] = text_series.apply(lambda x: sum(c.isdigit() for c in str(x)))\n",
        "    return features\n",
        "\n",
        "print(\"Extracting features...\")\n",
        "text_features = extract_features(df['Grievance Description'])\n",
        "\n",
        "if 'Category' in df.columns:\n",
        "    print(\"Using category information...\")\n",
        "    category_dummies = pd.get_dummies(df['Category'], prefix='category')\n",
        "    text_features = pd.concat([text_features, category_dummies], axis=1)\n",
        "\n",
        "def augment_data(df, target_count_per_class=300):\n",
        "    augmented_df = df.copy()\n",
        "    for urgency_level in df[\"Urgency Level\"].unique():\n",
        "        class_df = df[df[\"Urgency Level\"] == urgency_level]\n",
        "        current_count = len(class_df)\n",
        "        if current_count < target_count_per_class:\n",
        "            samples_needed = target_count_per_class - current_count\n",
        "            new_samples = []\n",
        "            for i in range(samples_needed):\n",
        "                sample = class_df.sample(1).iloc[0]\n",
        "                description = sample['Grievance Description']\n",
        "                words = description.split()\n",
        "                if len(words) > 3:\n",
        "                    idx = random.randint(0, len(words) - 1)\n",
        "                    if words[idx].lower() in ['problem', 'issue']:\n",
        "                        words[idx] = random.choice(['concern', 'trouble', 'matter'])\n",
        "                    augmented_text = ' '.join(words)\n",
        "                    new_sample = sample.copy()\n",
        "                    new_sample['Grievance Description'] = augmented_text\n",
        "                    new_samples.append(new_sample)\n",
        "            augmented_df = pd.concat([augmented_df, pd.DataFrame(new_samples)], ignore_index=True)\n",
        "    return augmented_df\n",
        "\n",
        "print(\"Augmenting data...\")\n",
        "df = augment_data(df)\n",
        "\n",
        "print(\"Preprocessing text after augmentation...\")\n",
        "df['processed_text'] = df['Grievance Description'].apply(preprocess_text)\n",
        "\n",
        "print(\"Extracting features after augmentation...\")\n",
        "text_features = extract_features(df['Grievance Description'])\n",
        "\n",
        "print(\"Vectorizing text...\")\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "text_vectors = tfidf.fit_transform(df['processed_text']).toarray()\n",
        "\n",
        "# Ensure both text_vectors and text_features have the same number of rows\n",
        "print(f\"text_vectors shape: {text_vectors.shape}, text_features shape: {text_features.shape}\")\n",
        "\n",
        "X = np.hstack((text_vectors, text_features))\n",
        "y = df['urgency_encoded']\n",
        "\n",
        "'''\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "print(\"Applying SMOTE...\")\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"Training models...\")\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "nb = MultinomialNB()\n",
        "lr = LogisticRegression(max_iter=200, random_state=42)\n",
        "svm = LinearSVC(random_state=42)\n",
        "\n",
        "ensemble = VotingClassifier(\n",
        "    estimators=[('rf', rf), ('gb', gb), ('nb', nb), ('lr', lr), ('svm', svm)], voting='hard'\n",
        ")\n",
        "\n",
        "ensemble.fit(X_train, y_train)'''\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "print(\"Scaling features to non-negative range...\")\n",
        "scaler = MinMaxScaler()\n",
        "X = scaler.fit_transform(X)  # Ensures all values are >= 0\n",
        "\n",
        "print(\"Applying SMOTE...\")\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)\n",
        "\n",
        "print(\"Training models...\")\n",
        "ensemble = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "        ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42)),\n",
        "        ('nb', MultinomialNB()),  # Now works because X is non-negative\n",
        "        ('lr', LogisticRegression(max_iter=500)),\n",
        "        ('svm', LinearSVC())\n",
        "    ],\n",
        "    voting='hard'\n",
        ")\n",
        "\n",
        "ensemble.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "print(\"Evaluating model...\")\n",
        "y_pred = ensemble.predict(X_test)\n",
        "\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOHI9UQ_c7FV",
        "outputId": "8848e9e0-5f19-4ead-cf09-b4c5330608a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preparing data...\n",
            "Original class distribution:\n",
            "Urgency Level\n",
            "medium    298\n",
            "high      209\n",
            "low       196\n",
            "Name: count, dtype: int64\n",
            "Preprocessing text...\n",
            "Extracting features...\n",
            "Using category information...\n",
            "Augmenting data...\n",
            "Preprocessing text after augmentation...\n",
            "Extracting features after augmentation...\n",
            "Vectorizing text...\n",
            "text_vectors shape: (900, 2856), text_features shape: (900, 8)\n",
            "Scaling features to non-negative range...\n",
            "Applying SMOTE...\n",
            "Training models...\n",
            "Evaluating model...\n",
            "Accuracy: 0.6500\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "        high       0.66      0.75      0.70        60\n",
            "         low       0.69      0.78      0.73        60\n",
            "      medium       0.57      0.42      0.48        60\n",
            "\n",
            "    accuracy                           0.65       180\n",
            "   macro avg       0.64      0.65      0.64       180\n",
            "weighted avg       0.64      0.65      0.64       180\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            " [[45  6  9]\n",
            " [ 3 47 10]\n",
            " [20 15 25]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vvd3G764fj0y",
        "outputId": "8b60b2c0-e162-497a-d7a6-cc88e979dc4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.7-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.13.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.0.0)\n",
            "Downloading catboost-1.2.7-cp311-cp311-manylinux2014_x86_64.whl (98.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import nltk\n",
        "import re\n",
        "import warnings\n",
        "import random\n",
        "from collections import Counter\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Ensure NLTK resources are available\n",
        "nltk.download('vader_lexicon', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "print(\"Loading and preparing data...\")\n",
        "df = pd.read_csv(\"proc_ds.csv\", encoding=\"ISO-8859-1\")\n",
        "df = df.dropna(subset=['Grievance Description', 'Urgency Level'])\n",
        "df[\"Urgency Level\"] = df[\"Urgency Level\"].str.lower()\n",
        "df = df[df[\"Urgency Level\"].isin([\"low\", \"medium\", \"high\"])]\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "df[\"urgency_encoded\"] = label_encoder.fit_transform(df[\"Urgency Level\"])\n",
        "\n",
        "print(\"Original class distribution:\")\n",
        "print(df[\"Urgency Level\"].value_counts())\n",
        "\n",
        "# Text Preprocessing\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text.lower())\n",
        "    return text\n",
        "\n",
        "print(\"Preprocessing text...\")\n",
        "df['processed_text'] = df['Grievance Description'].apply(preprocess_text)\n",
        "\n",
        "# Feature Engineering\n",
        "print(\"Extracting features...\")\n",
        "def extract_features(text_series):\n",
        "    features = pd.DataFrame()\n",
        "    features['text_length'] = text_series.apply(len)\n",
        "    features['word_count'] = text_series.apply(lambda x: len(str(x).split()))\n",
        "    features['exclamation_count'] = text_series.apply(lambda x: str(x).count('!'))\n",
        "    return features\n",
        "\n",
        "text_features = extract_features(df['Grievance Description'])\n",
        "\n",
        "# Sentiment Score\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "df['sentiment_score'] = df['Grievance Description'].apply(lambda x: sia.polarity_scores(str(x))['compound'])\n",
        "\n",
        "# Add Sentiment Score to Features\n",
        "text_features['sentiment_score'] = df['sentiment_score']\n",
        "\n",
        "# Data Augmentation\n",
        "def augment_data(df, target_count_per_class=300):\n",
        "    augmented_df = df.copy()\n",
        "    for urgency_level in df[\"Urgency Level\"].unique():\n",
        "        class_df = df[df[\"Urgency Level\"] == urgency_level]\n",
        "        current_count = len(class_df)\n",
        "        if current_count < target_count_per_class:\n",
        "            samples_needed = target_count_per_class - current_count\n",
        "            new_samples = [class_df.sample(1, replace=True).iloc[0] for _ in range(samples_needed)]\n",
        "            augmented_df = pd.concat([augmented_df, pd.DataFrame(new_samples)], ignore_index=True)\n",
        "    return augmented_df\n",
        "\n",
        "\n",
        "print(\"Augmenting data...\")\n",
        "df = augment_data(df)\n",
        "\n",
        "print(\"Preprocessing text after augmentation...\")\n",
        "df['processed_text'] = df['Grievance Description'].apply(preprocess_text)\n",
        "\n",
        "print(\"Extracting features after augmentation...\")\n",
        "text_features = extract_features(df['Grievance Description'])\n",
        "text_features['sentiment_score'] = df['sentiment_score']\n",
        "\n",
        "print(\"Vectorizing text with N-grams...\")\n",
        "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 3))\n",
        "text_vectors = tfidf.fit_transform(df['processed_text']).toarray()\n",
        "\n",
        "# Feature Scaling\n",
        "scaler = StandardScaler()\n",
        "X_text_features = scaler.fit_transform(text_features)\n",
        "\n",
        "# Merge All Features\n",
        "X = np.hstack((text_vectors, X_text_features))\n",
        "y = df['urgency_encoded']\n",
        "\n",
        "print(\"Applying SMOTE...\")\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)\n",
        "\n",
        "# Model Training\n",
        "print(\"Training models...\")\n",
        "stacked_model = StackingClassifier(\n",
        "    estimators=[\n",
        "        ('xgb', XGBClassifier(n_estimators=200, learning_rate=0.1, max_depth=5)),\n",
        "        ('cat', CatBoostClassifier(iterations=200, depth=5, learning_rate=0.1, verbose=0)),\n",
        "        ('rf', RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "    ],\n",
        "    final_estimator=GradientBoostingClassifier(n_estimators=100, learning_rate=0.1),\n",
        "    cv=5\n",
        ")\n",
        "\n",
        "stacked_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluation\n",
        "print(\"Evaluating model...\")\n",
        "y_pred = stacked_model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECbfGlFzeVZ4",
        "outputId": "f6c62a0c-ac14-4222-8de2-8aaee608ebfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preparing data...\n",
            "Original class distribution:\n",
            "Urgency Level\n",
            "medium    298\n",
            "high      209\n",
            "low       196\n",
            "Name: count, dtype: int64\n",
            "Preprocessing text...\n",
            "Extracting features...\n",
            "Augmenting data...\n",
            "Preprocessing text after augmentation...\n",
            "Extracting features after augmentation...\n",
            "Vectorizing text with N-grams...\n",
            "Applying SMOTE...\n",
            "Training models...\n",
            "Evaluating model...\n",
            "Accuracy: 0.6722\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        high       0.83      0.57      0.67        60\n",
            "         low       0.73      0.75      0.74        60\n",
            "      medium       0.55      0.70      0.61        60\n",
            "\n",
            "    accuracy                           0.67       180\n",
            "   macro avg       0.70      0.67      0.67       180\n",
            "weighted avg       0.70      0.67      0.67       180\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[34  5 21]\n",
            " [ 1 45 14]\n",
            " [ 6 12 42]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deep_translator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9_nM0I0g60K",
        "outputId": "f05e36b3-f43c-48cf-e546-650c8054884b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deep_translator\n",
            "  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.11/dist-packages (from deep_translator) (4.13.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from deep_translator) (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2025.1.31)\n",
            "Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: deep_translator\n",
            "Successfully installed deep_translator-1.11.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import random\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from textblob import TextBlob\n",
        "from deep_translator import GoogleTranslator\n",
        "\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Function to paraphrase text by replacing synonyms\n",
        "def paraphrase_text(text):\n",
        "    words = text.split()\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        synonyms = wordnet.synsets(word)\n",
        "        if synonyms:\n",
        "            new_word = synonyms[0].lemmas()[0].name()\n",
        "            new_words.append(new_word)\n",
        "        else:\n",
        "            new_words.append(word)\n",
        "    return \" \".join(new_words)\n",
        "\n",
        "# Function for back-translation (English → French → English)\n",
        "def back_translate(text):\n",
        "    french_text = GoogleTranslator(source=\"auto\", target=\"fr\").translate(text)\n",
        "    return GoogleTranslator(source=\"fr\", target=\"en\").translate(french_text)\n",
        "\n",
        "# Data Augmentation\n",
        "def augment_data(df, target_count_per_class=400):\n",
        "    augmented_df = df.copy()\n",
        "    for urgency_level in df[\"Urgency Level\"].unique():\n",
        "        class_df = df[df[\"Urgency Level\"] == urgency_level]\n",
        "        current_count = len(class_df)\n",
        "        if current_count < target_count_per_class:\n",
        "            samples_needed = target_count_per_class - current_count\n",
        "            new_samples = []\n",
        "            for _ in range(samples_needed):\n",
        "                sample = class_df.sample(1, replace=True).iloc[0].copy()\n",
        "                if random.random() > 0.5:\n",
        "                    sample[\"Grievance Description\"] = paraphrase_text(sample[\"Grievance Description\"])\n",
        "                else:\n",
        "                    sample[\"Grievance Description\"] = back_translate(sample[\"Grievance Description\"])\n",
        "                new_samples.append(sample)\n",
        "            augmented_df = pd.concat([augmented_df, pd.DataFrame(new_samples)], ignore_index=True)\n",
        "    return augmented_df\n",
        "\n",
        "# Feature Extraction\n",
        "def extract_features(df):\n",
        "    df[\"text_length\"] = df[\"Grievance Description\"].apply(len)\n",
        "    df[\"num_words\"] = df[\"Grievance Description\"].apply(lambda x: len(x.split()))\n",
        "    df[\"sentiment\"] = df[\"Grievance Description\"].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
        "    return df\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"proc_ds.csv\", encoding=\"ISO-8859-1\")\n",
        "\n",
        "# Augment Data\n",
        "df = augment_data(df)\n",
        "\n",
        "# Feature Extraction\n",
        "df = extract_features(df)\n",
        "\n",
        "# Encode Labels\n",
        "df[\"urgency_encoded\"] = df[\"Urgency Level\"].map({\"low\": 0, \"medium\": 1, \"high\": 2})\n",
        "\n",
        "# Vectorize Text\n",
        "tfidf = TfidfVectorizer(ngram_range=(1, 5), analyzer=\"char_wb\")\n",
        "text_vectors = tfidf.fit_transform(df[\"Grievance Description\"]).toarray()\n",
        "\n",
        "# Scale Features\n",
        "scaler = StandardScaler()\n",
        "text_features = scaler.fit_transform(df[[\"text_length\", \"num_words\", \"sentiment\"]])\n",
        "\n",
        "# Combine Features\n",
        "X = np.hstack((text_vectors, text_features))\n",
        "y = df[\"urgency_encoded\"]\n",
        "\n",
        "# Apply SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "# Train/Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define Models\n",
        "base_models = [\n",
        "    (\"rf\", RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)),\n",
        "    (\"xgb\", XGBClassifier(n_estimators=200, max_depth=5, learning_rate=0.1, random_state=42)),\n",
        "]\n",
        "\n",
        "# Stacking Classifier\n",
        "stacking_model = StackingClassifier(estimators=base_models, final_estimator=GradientBoostingClassifier(n_estimators=100))\n",
        "\n",
        "# Train Model\n",
        "stacking_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate Model\n",
        "accuracy = stacking_model.score(X_test, y_test)\n",
        "print(f\"🚀 Final Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUc95W94fhsm",
        "outputId": "ecc57d39-aaed-4726-e453-a9ce260a9527"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Final Accuracy: 0.7292\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Define filenames\n",
        "model_filename = \"stacking_model_prior_72.pkl\"\n",
        "tfidf_filename = \"tfidf_vectorizer_prior_72.pkl\"\n",
        "scaler_filename = \"scaler_prior_72.pkl\"\n",
        "encoder_filename = \"urgency_encoder_prior_72.pkl\"\n",
        "\n",
        "# Save Stacking Model\n",
        "with open(model_filename, \"wb\") as model_file:\n",
        "    pickle.dump(stacking_model, model_file)\n",
        "\n",
        "# Save TF-IDF Vectorizer\n",
        "with open(tfidf_filename, \"wb\") as tfidf_file:\n",
        "    pickle.dump(tfidf, tfidf_file)\n",
        "\n",
        "# Save Standard Scaler\n",
        "with open(scaler_filename, \"wb\") as scaler_file:\n",
        "    pickle.dump(scaler, scaler_file)\n",
        "\n",
        "# Save Label Encoder (Urgency Mapping)\n",
        "urgency_mapping = {\"low\": 0, \"medium\": 1, \"high\": 2}\n",
        "with open(encoder_filename, \"wb\") as encoder_file:\n",
        "    pickle.dump(urgency_mapping, encoder_file)\n",
        "\n",
        "print(\"✅ Model and necessary components saved successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gB4G30s5g3cW",
        "outputId": "affb6310-ff0e-4313-f0ae-26370e6d6568"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model and necessary components saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zT99E_uloLug"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}